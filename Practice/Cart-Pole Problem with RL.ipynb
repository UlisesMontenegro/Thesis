{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym #Open AI library\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 #Learning Rate\n",
    "gamma = 0.95 #Discount Factor\n",
    "\n",
    "epochs = 60000 #How many iterations\n",
    "total_time = 0\n",
    "total_reward = 0\n",
    "prev_reward = 0\n",
    "\n",
    "Observation = [30,30,50,50]\n",
    "step_size = np.array([.25,.25, .01, .01])\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_decay_value = 0.9995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(Observation+[env.action_space.n])) #Randomly initializing Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method used to discretize the state space given by OpanAI Gym library\n",
    "def discrete_state(state):\n",
    "    aux = state/step_size + np.array([15,12,1,10])\n",
    "    return tuple(aux.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs+1):\n",
    "    t_initial = time.time()\n",
    "\n",
    "    discrete_state_var = discrete_state(env.reset()[0]) #Get the discrete state for the restarted environment, so we know what's going on\n",
    "\n",
    "    done = False #Control boolean\n",
    "\n",
    "    epoch_reward = 0\n",
    "\n",
    "    #print(\"Starting...\")\n",
    "\n",
    "    if epoch % 1000 == 0: #I'm going to print every each 1000 epochs\n",
    "        print(\"Episode: \" + str(epoch))\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon: #If some random number is greater than epsilon\n",
    "            action = np.argmax(q_table[discrete_state_var]) #Look into the Q-Table for the action that maximizes the reward for the actual state (Exploitation)\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n) #Pick a random action from the action space (Exploration)\n",
    "            print(\"Action picked randomly\")\n",
    "    \n",
    "        print(\"Action: \" + str(action))\n",
    "        new_state, reward, terminated, truncated, done = env.step(action) #Update the environment\n",
    "\n",
    "        epoch_reward += reward\n",
    "        \n",
    "        print(\"Q Table: \" + str(q_table))\n",
    "        print(\"New State: \" + str(new_state))\n",
    "        new_discrete_state = discrete_state(new_state)\n",
    "        print(\"New Discrete State: \" + str(new_discrete_state))\n",
    "\n",
    "        if epoch % 1000 == 0: #I'm rendering the environment every each 1000 epochs\n",
    "            env.render()\n",
    "        \n",
    "        if not done: #If the game is not over, update the Q-Table\n",
    "            max_new_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state_var + (action,)]\n",
    "            new_q = current_q + lr*(reward + (gamma*max_new_q) - current_q)\n",
    "            q_table[discrete_state_var + (action,)] = new_q\n",
    "\n",
    "        discrete_state_var = new_discrete_state #Updating the state\n",
    "\n",
    "        if epsilon > 0.05:\n",
    "            if epoch_reward > prev_reward and epoch > 10000:\n",
    "                epsilon = math.pow(epsilon_decay_value, epoch-10000)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "        #Calculating total times\n",
    "        tfinal = time.time()\n",
    "        episode_total_time = tfinal - t_initial\n",
    "        total_time += episode_total_time\n",
    "\n",
    "        #Calculating total rewards\n",
    "        total_reward += epoch_reward\n",
    "        prev_reward = epoch_reward\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            mean_time = total_time/1000\n",
    "            print(\"Average Time: \" + str(mean_time))\n",
    "            total_time = 0\n",
    "            mean_reward = total_reward/1000\n",
    "            print(\"Average Reward: \" + str(mean_reward))\n",
    "            total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
