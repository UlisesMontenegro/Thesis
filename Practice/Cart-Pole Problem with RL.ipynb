{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym #Open AI library\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 #Learning Rate\n",
    "gamma = 0.95 #Discount Factor\n",
    "\n",
    "epochs = 60000 #How many iterations\n",
    "total_time = 0\n",
    "total_reward = 0\n",
    "prev_reward = 0\n",
    "\n",
    "Observation = [30,30,50,50]\n",
    "step_size = np.array([.25,.25, .01, .01])\n",
    "\n",
    "epsilon = 1\n",
    "epsilon_decay_value = 0.9995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=0, high=1, size=(Observation+[env.action_space.n])) #Randomly initializing Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method used to discretize the state space given by OpanAI Gym library\n",
    "def discrete_state(state):\n",
    "    discrete_state = (state)/step_size + np.array([15,12,1,10])\n",
    "    return tuple(discrete_state.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(env.reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Episode: 0\n",
      "[ 0.02351694 -0.16924775  0.02265133  0.32280067]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_state)\n\u001b[0;32m     24\u001b[0m epoch_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 26\u001b[0m new_discrete_state \u001b[38;5;241m=\u001b[39m \u001b[43mdiscrete_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m#I'm rendering the environment every each 1000 epochs\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs+1):\n",
    "    t_initial = time.time()\n",
    "\n",
    "    discrete_state = discrete_state(env.reset()[0]) #Get the discrete state for the restarted environment, so we know what's going on\n",
    "\n",
    "    done = False #Control boolean\n",
    "\n",
    "    epoch_reward = 0\n",
    "\n",
    "    #print(\"Starting...\")\n",
    "\n",
    "    if epoch % 1000 == 0: #I'm going to print every each 1000 epochs\n",
    "        print(\"Episode: \" + str(epoch))\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon: #If some random number is greater than epsilon\n",
    "            action = np.argmax(q_table[discrete_state]) #Look into the Q-Table for the action that maximizes the reward for the actual state (Exploitation)\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n) #Pick a random action from the action space (Exploration)\n",
    "    \n",
    "        new_state, reward, terminated, truncated, done = env.step(action) #Update the environment\n",
    "\n",
    "        epoch_reward += reward\n",
    "        \n",
    "        new_discrete_state = discrete_state(new_state)\n",
    "\n",
    "        if epoch % 1000 == 0: #I'm rendering the environment every each 1000 epochs\n",
    "            env.render()\n",
    "        \n",
    "        if not done: #If the game is not over, update the Q-Table\n",
    "            max_new_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = current_q + lr*(reward + (gamma*max_new_q) - current_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state #Updating the state\n",
    "\n",
    "        if epsilon > 0.05:\n",
    "            if epoch_reward > prev_reward and epoch > 10000:\n",
    "                epsilon = math.pow(epsilon_decay_value, epoch-10000)\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "        #Calculating total times\n",
    "        tfinal = time.time()\n",
    "        episode_total_time = tfinal - t_initial\n",
    "        total_time += episode_total_time\n",
    "\n",
    "        #Calculating total rewards\n",
    "        total_reward += epoch_reward\n",
    "        prev_reward = epoch_reward\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            mean_time = total_time/1000\n",
    "            print(\"Average Time: \" + str(mean_time))\n",
    "            total_time = 0\n",
    "            mean_reward = total_reward/1000\n",
    "            print(\"Average Reward: \" + str(mean_reward))\n",
    "            total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
